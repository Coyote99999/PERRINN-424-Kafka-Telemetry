version: '3.8'

networks:
  kafka-network:
    driver: bridge

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.0.0
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASS}@airflow_postgres:5432/${AIRFLOW_DB_NAME}
    - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__LOGGING_LEVEL=INFO
  volumes:
    - ./airflow-dags:/opt/airflow/dags
    - airflow-logs:/opt/airflow/logs
    - airflow-plugins:/opt/airflow/plugins
    - ./airflow-config.cfg:/opt/airlfow/airflow.cfg
  depends_on:
    - airflow_postgres
  networks:
    - kafka-network


services:

  # Responsible for coordination and synchronization between distributed systems, such as Kafka and HBase.
  zookeeper:
    image: ubuntu/zookeeper:3.8-22.04_edge
    container_name: zookeeper
    networks:
      - kafka-network
  # Provides a web-based user interface for monitoring and managing Zookeeper clusters.
  zoonavigator:
    image: elkozmon/zoonavigator:latest
    container_name: zoonavigator
    networks:
      - kafka-network
    ports:
      - "${ZOONAVIGATOR_UI_PORT}:9000"
    environment:
      HTTP_PORT: 9000
      CONNECTION_LOCALZK_NAME: "Local ZooKeeper"
      CONNECTION_LOCALZK_CONN: "zookeeper:2181"
    depends_on:
      - zookeeper

 # A distributed streaming platform used for real-time data ingestion and processing.
  kafka:
    image: ubuntu/kafka:3.6-22.04_edge
    container_name: kafka
    networks:
      - kafka-network
    ports:
      - "${KAFKA_PORT}:9092"
    volumes:
      - ./kafka-server.properties:/etc/kafka/server.properties
    depends_on:
      - zookeeper

  # Offers a user interface for monitoring Kafka clusters and topics.
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    networks:
      - kafka-network
    ports:
      - "${KAFKA_UI_PORT}:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: default
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka

  # Offers real time stream processing, as opposed to spark which is used for batch processing
  flink-jobmanager:
    build:
      context: .
      dockerfile: flinkDockerfile
    container_name: flink-jobmanager
    networks:
      - kafka-network
    ports:
      - "${FLINK_UI_PORT}:8081"
      - "${FLINK_RPC_PORT_JOBMANAGER}:6123"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager

  flink-taskmanager:
    build:
      context: .
      dockerfile: flinkDockerfile
    container_name: flink-taskmanager
    networks:
      - kafka-network
    command: taskmanager
    ports:
      - "${FLINK_RPC_PORT_TASKMANAGER}:6122"
      - "${FLINK_DATA_PORT_TASKMANAGER}:6121"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    depends_on:
      - flink-jobmanager

  # Acts as the Spark master node, managing the allocation of resources and scheduling tasks across the Spark cluster.

  spark-master:
    image: bitnami/spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    networks:
      - kafka-network
    ports:
      - "${SPARK_UI_PORT}:8080"  # Spark UI
      - "${SPARK_MASTER_PORT}:7077"  # Spark Master
    volumes:
      - spark-logs:/opt/spark/logs

  spark-worker:
    image: bitnami/spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - kafka-network
    depends_on:
      - spark-master
    volumes:
      - spark-logs:/opt/spark/logs

  # A distributed NoSQL database that provides real-time read/write access to large datasets, often used for random, real-time access to big data.
  hbase:
    image: hyness/hbase-rest-standalone
    container_name: hbase
    networks:
      - kafka-network
    volumes:
      - ./hbase-site.xml:/opt/hbase/conf/hbase-site.xml  # Mount custom configuration directory
      - hbase_data:/data/hbase
    ports:
     # - "2181:2181" #Zookeeper port, no need for it as we connect hbase to an external zookeeper
      - "${HBASE_REST_API_PORT}:8080" # REST api port, can interact with hbase api through here
      - "${HBASE_MASTER_PORT}:16000" # hbase master port
      - "${HBASE_UI_PORT}:16010" # UI port
      - "${HBASE_REGIONSERVER_PORT}:16020" # regionserver port, used by hbase region servers for handling read and write requests from clients.
      - "${HBASE_REGIONSERVER_UI_PORT}:16030" # regionserver ui port
    environment:
      - HBASE_ZOOKEEPER_QUORUM=zookeeper
      - HBASE_ZOOKEEPER_CLIENT_PORT=2181

 # Apache Ozone section

  datanode:
    container_name: ozone-datanode
    image: apache/ozone:1.4.0
    ports:
      - "${OZONE_DATANODE_PORT}:9864"
    command: ["ozone","datanode"]
    env_file:
      - ./ozone-config.env
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  om:
    image: apache/ozone:1.4.0
    container_name: ozone-om
    ports:
      - "${OZONE_OM_UI_PORT}:9874"
    environment:
      ENSURE_OM_INITIALIZED: /data/metadata/om/current/VERSION
      WAITFOR: scm:9876
    env_file:
      - ./ozone-config.env
    command: ["ozone","om"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  scm:
    image: apache/ozone:1.4.0
    container_name: ozone-scm
    ports:
      - "${OZONE_SCM_UI_PORT}:9876"
    env_file:
      - ./ozone-config.env
    environment:
      ENSURE_SCM_INITIALIZED: /data/metadata/scm/current/VERSION
    command: ["ozone","scm"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  recon:
    image: apache/ozone:1.4.0
    container_name: ozone-recon
    ports:
      - "${OZONE_RECON_UI_PORT}:9888"
    env_file:
      - ./ozone-config.env
    command: ["ozone","recon"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml

  s3g:
    image: apache/ozone:1.4.0
    container_name: ozone-s3g
    ports:
      - "${OZONE_S3G_S3_GATEWAY_ENDPOINT_PORT}:9878"
    env_file:
      - ./ozone-config.env
    command: ["ozone","s3g"]
    networks:
      - kafka-network
    volumes:
      - ./ozone-site.xml:/opt/hadoop/etc/hadoop/ozone-site.xml


  airflow_postgres:
    image: postgres
    container_name: airflow_postgres
    environment:
      - POSTGRES_USER=${AIRFLOW_DB_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_DB_PASS}
      - POSTGRES_DB=${AIRFLOW_DB_NAME}
      - POSTGRES_PORT=5432
    networks:
      - kafka-network

  airflow-init:
    << : *airflow-common
    container_name: airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - airflow users list || ( airflow db init &&
        airflow users create
          --role Admin
          --username ${AIRFLOW_ADMIN_USERNAME}
          --password ${AIRFLOW_ADMIN_PASSWORD}
          --email ${AIRFLOW_ADMIN_EMAIL}
          --firstname ${AIRFLOW_ADMIN_FIRSTNAME}
          --lastname ${AIRFLOW_ADMIN_LASTNAME} )
    restart: on-failure

  airflow-webserver:
    << : *airflow-common
    command: airflow webserver
    ports:
      - "${AIRFLOW_PORT}:8080"
    container_name: airflow_webserver
    restart: always

  airflow-scheduler:
    << : *airflow-common
    command: airflow scheduler
    container_name: airflow_scheduler
    restart: always


  # Apache Hadoop Section (HDFS deprecated in favor of Ozone, todo - switch YARN cluster to Ozone at some point if needed)

  # NOTE: Hadoop itself includes HDFS (Hadoop Distributed File System) as one of its core components. 
  # NOTE: When you install Hadoop, you automatically get HDFS along with it. 
  # NOTE: HDFS is the primary storage layer in the Hadoop ecosystem, providing distributed storage for large-scale data processing. 
  # NOTE: So, you don't need to separately install HDFS when setting up a Hadoop cluster; it comes bundled with the Hadoop distribution.

  # Manages the filesystem namespace and metadata for HDFS (Hadoop Distributed File System). HDFS Component
#  namenode:
#    image: apache/hadoop:3
#    container_name: hadoop-namenode
#    hostname: namenode
#    command: ["hdfs", "namenode"]
#    ports:
#      - "${HDFS_NAMENODE_UI_PORT}:9870"
#    env_file:
#      - ./hadoop-config.env
#    environment:
#      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"


  # Stores actual data blocks and serves read and write requests from clients in HDFS. HDFS Component
#  datanode:
#    image: apache/hadoop:3
#    container_name: hadoop-datanode
#    command: ["hdfs", "datanode"]
#    env_file:
#      - ./hadoop-config.env

  # Manages resources in the YARN (Yet Another Resource Negotiator) cluster, allocating resources to applications.
#  resourcemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-resourcemanager
#    hostname: resourcemanager
#    command: ["yarn", "resourcemanager"]
#    ports:
#       - "${HADOOP_RESOURCEMANAGER_UI_PORT}:8088"
#    env_file:
#      - ./hadoop-config.env
#    volumes:
#      - ./test.sh:/opt/test.sh

  # Manages resources and executes tasks on individual nodes in the YARN cluster.
#  nodemanager:
#    image: apache/hadoop:3
#    container_name: hadoop-nodemanager
#    command: ["yarn", "nodemanager"]
#    env_file:
#      - ./hadoop-config.env

  # The holy grail of monitoring - with this we can have out real time graphs
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    networks:
      - kafka-network
    ports:
      - "${GRAFANA_UI_PORT}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
    volumes:
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards

 # Machine Learning Tools

   # MLFlow Tracking Server to track experiments for ML
  tracking_server:
    restart: always
    build:
      context: .
      dockerfile: mlflowDockerfile
    container_name: mlflow_server
    depends_on:
      - mlflow_db
      - s3
    ports:
      - "${MLFLOW_PORT}:5000"
    networks:
      - kafka-network
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT}
      - MLFLOW_S3_IGNORE_TLS=true
    command: >
      mlflow server
      --backend-store-uri postgresql://${PG_USER}:${PG_PASSWORD}@mlflow_db:${PG_PORT}/${PG_DATABASE}
      --host 0.0.0.0
      --serve-artifacts
      --artifacts-destination s3://${MLFLOW_BUCKET_NAME}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/"]
      interval: 30s
      timeout: 10s
      retries: 3

  mlflow_db:
    restart: always
    image: postgres
    container_name: mlflow_db
    expose:
      - "${PG_PORT}"
    networks:
      - kafka-network
    environment:
      - POSTGRES_USER=${PG_USER}
      - POSTGRES_PASSWORD=${PG_PASSWORD}
      - POSTGRES_DATABASE=${PG_DATABASE}
    volumes:
      - mlflow_db_data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "${PG_PORT}", "-U", "${PG_USER}"]
      interval: 5s
      timeout: 5s
      retries: 3

  # Now we add the pgadmin4 service so we can monitor db contents should we need to
  pgadmin4:
    image: dpage/pgadmin4:latest
    container_name: pgadmin4
    networks:
      - kafka-network
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}
    ports:
      - "${PGADMIN4_PORT}:80"
    volumes:
      - ./pgadmin4_psql_servers.json:/pgadmin4/servers.json

  # S3 storage for MLFlow
  s3:
    restart: always
    image: minio/minio:latest
    container_name: mlflow_minio
    volumes:
      - minio_data:/data
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    networks:
      - kafka-network
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ADDRESS=${MINIO_ADDRESS}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_STORAGE_USE_HTTPS=${MINIO_STORAGE_USE_HTTPS}
      - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
    command: server --console-address ":${MINIO_CONSOLE_PORT}" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  s3_mlflow_createbuckets:
    container_name: mlflow_create_bucket_job
    image: minio/mc
    depends_on:
      - s3
    networks:
      - kafka-network
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb s3minio/${MLFLOW_BUCKET_NAME};
      /usr/bin/mc policy set public s3minio/${MLFLOW_BUCKET_NAME};
      exit 0;
      "

  s3_mlflow_createaccesskey:
    container_name: mlflow_create_accesskey_job
    image: minio/mc
    depends_on:
      - s3
    networks:
      - kafka-network
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set s3minio http://s3:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc admin user svcacct add --access-key "${MINIO_ACCESS_KEY}" --secret-key "${MINIO_SECRET_ACCESS_KEY}" --name mlflow --description connection s3minio ${MINIO_ROOT_USER};
      exit 0;
      "
volumes:
  mlflow_db_data:
  minio_data:
  spark-logs:
  hbase_data:
  airflow-logs:
  airflow-plugins:
